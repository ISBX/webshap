tool:
  - >
    Machine learning (ML) has been integrated into our everyday Web experience
    from social media ranking to fraud detection. For better privacy and lower
    latency, researchers and developers have made strides in developing Web ML
    models that can run on clients' edge devices. To reflect the "transparency
    and explainability" <a
    href='https://www.w3.org/TR/webmachinelearning-ethics/#principle-6-transparency-and-explainability'
    target='_blank' rel='noreferrer'>ethical principle</a> of Web ML, <a
    href='https://github.com/poloclub/webshap' target='_blank'
    rel='noreferrer'>WebSHAP</a> is the first JavaScript library that brings <a
    href='https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html'
    target='_blank' rel='noreferrer'>Kernel SHAP</a> to the Web
    environment‚Äîenabling developers and end-users to <strong>explain any ML
    models anywhere.</strong>

  - >
    Kernel SHAP is the state-of-the-art model-agnostic explainability technique
    to help ML practitioners and end-users understand how ML models make
    predictions. For any given ML model, Kernel SHAP computes the contribution
    score of each input feature. Developed with modern web technologies, such as
    <em>WebGL</em> and <em>Web Workers</em>, WebSHAP empowers users to run
    Kernel SHAP directly in their browsers, providing a
    <strong>ubiquitous</strong>, <strong>private</strong>, and
    <strong>interactive</strong> explanation experience.

usage:
  p1: >
    WebSHAP supports both browser and Node.js environments. To install WebSHAP,
    you can use npm: <code>pip install WebSHAP</code>. Then, you just need to
    call one function to explain the predictions of your ML model in the
    browser. To get started, see <a
    href='https://github.com/poloclub/webshap#getting-started' target='_blank'
    rel='noreferrer'>this tutorial</a> and the <a
    href='https://poloclub.github.io/webshap/doc/' target='_blank'
    rel='noreferrer'>documentation</a>.

tutorial:
  p1: >
    On this webpage, we showcase three diverse examples that apply WebSHAP to
    explain ML models in your browser. These examples use distinct model
    architectures (e.g., gradient-boosting, CNNs, and transformers) and data
    types (tabular, image, and text).
  items:
    - id: tabular
      name: Explaining XGBoost
      isWide: false
      descriptions:
        - >
          We present Loan Explainer as an example of applying WebSHAP to explain
          a financial ML model in browsers. For a live demo of Loan Explainer,
          click the <a href='./?model=text'>first tab</a> under the tool.
        - >
          This example showcases a bank using an <a
          href='https://github.com/dmlc/xgboost' target='_blank'
          rel='noreferrer'>XGBoost classifier</a> on the <a
          href='https://www.kaggle.com/datasets/wordsforthewise/lending-club'
          target='_blank' rel='noreferrer'>LendingClub</a> dataset to predict if
          a loan applicant will be able to repay the loan on time. With this
          model, the bank can make automatic loan approval decisions. It's
          important to understand how these high-stakes decisions are being
          made, and that's where WebSHAP comes in. It provides <em>private</em>,
          <em>ubiquitous</em>, and <em>interactive</em> ML explanations.
        - >
          This demo runs entirely on the client side, making it accessible from
          desktops, tablets, and phones. The model inference is powered by <a
          href='https://github.com/microsoft/onnxruntime' target='_blank'
          rel='noreferrer'>ONNX Runtime</a>. With Loan Explainer, users can
          experiment with different feature inputs and instantly see the model's
          predictions, along with clear explanations for those predictions.
      caption: The <em>Rashomon Overview</em> adapts Sunburst to summarize the
        whole Rashomon set. It organizes decision trees based on their decision
        paths. Users can click a sector to zoom into a specific subset of models
        with similar prediction patterns. Users can also control the number of
        levels to show in the Sunburst.
    - id: image
      name: Explaining Convolutional Neural Networks (CNNs)
      isWide: true
      descriptions:
        - >
          We apply WebSHAP to explain convolutional neural networks (CNNs) in
          browsers. For a live demo of this explainer,
          click the <a href='./?model=image'>second tab</a> under the tool.
        - >
          In this example, we first train a TinyVGG model to classify images
          into four categories: üêû<code>Ladybug</code>, ‚òïÔ∏è<code>Espresso</code>,
          üçä<code>Orange</code>, and üöô<code>Sports Car</code>. TinyVGG is a
          type of convolutional neural network. For more details about the model
          architecture, check out <a href='https://poloclub.github.io/cnn-explainer' target='_blank' rel='noreferrer'>CNN Explainer</a>. TinyVGG is implemented using
          <a href='https://www.tensorflow.org/js' target='_blank' rel='noreferrer'>TensorFlow.js</a>.
        - >
          To explain the predictions of TinyVGG, we first apply image
          segmentation (<a
          href='https://www.iro.umontreal.ca/~mignotte/IFT6150/Articles/SLIC_Superpixels.pdf'
          target='_blank' rel='noreferrer'>SLIC</a>) to divide the input image
          into multiple segments. Then, we compute SHAP scores on each segment
          for each class. The background data here are white pixels. We compute
          SHAP values for segments instead of raw pixels for computation
          efficiency. For example, in the figure above, there are only 16 input
          features (16 segments) for WebSHAP, but there would have been \(64 \times 64
          \times 3 = 12,288\) input features if we use raw pixels. Finally, we visualize
          the SHAP scores of each segment as an overlay with a diverging color
          scale on top of the original input image.
        - >
          Everything in this example (TinyVGG, image segmenter, WebSHAP) runs in
          the user's browser. WebSHAP enables <em>interactive</em>
          explanation: users can click a button to use a random input image or
          upload their own images. Both model inference and SHAP computation are
          real-time.
      caption: >
        A <em>Tree Windows</em> uses a node-link diagram to illustrate the
        details of a decision tree. An alternative waterfall-like chart
        augments node size with its sample size, helping users to better
        understand each tree's robustness. To compare the structure and
        prediction patterns of multiple decision trees, users can open multiple
        windows and drag to reposition them side by side.
    - id: text
      name: Explaining Transformers
      isWide: false
      descriptions:
        - >
          We use WebSHAP to explain the predictions of a Transformer text
          classifier in browsers. For a live demo of this explainer,
          click the <a href='./?model=text'>second tab</a> under the tool.
        - >
          We train an <a
          href='https://github.com/microsoft/xtreme-distil-transformers'
          target='_blank' rel='noreferrer'>XtremeDistil model</a> to predict if
          an input text is toxic. The model is a distilled version
          of pre-trained transformer-based language model BERT. We train this
          model on the <a
          href='https://www.kaggle.com/competitions/jigsaw-toxic-comment-classification-challenge/data'
          target='_blank' rel='noreferrer'>Toxic Comments dataset</a>. Then, we
          quantize and export the trained model to use <code>int8</code> weights with <a
          href='https://github.com/onnx/onnxmltools' target='_blank'
          rel='noreferrer'>ONNX</a>. We use <a
          href='https://github.com/tensorflow/tfjs-models/blob/master/qna/src/bert_tokenizer.ts'
          target='_blank' rel='noreferrer'>TensorFlow.js</a> for tokenization
          and <a href='https://onnxruntime.ai/' target='_blank'
          rel='noreferrer'>ONNX Runtime</a> for model inference.
        - >
          To explain the model's predictions, we compute SHAP scores for each
          input token. For background data, we use BERT's attention mechanism to
          mask tokens. For example, we represent a "missing" token by setting
          its <a
          href='https://huggingface.co/docs/transformers/glossary#attention-mask'
          target='_blank' rel='noreferrer'>attention map</a> to <code>0</code>,
          which tells the model to ignore this token. Finally, we visualize the
          SHAP scores as token's background color with a diverging color scale.
        - >
          All components in this example (XtremeDistil, tokenizer, WebSHAP) runs
          on the client-side. WebSHAP provides provides <em>private</em>,
          <em>ubiquitous</em>, and <em>interactive</em> explanations. Users can
          edit the input text and see new predictions and explanations. The
          model inference is real-time, and SHAP computation takes about 5
          seconds for 50 tokens.
      caption: >
        The <em>Search Panel</em> allows users to search for decision trees with
        desired properties by applying different filtering tools. For example, a
        user can drag the accuracy slider to hide decision trees with slightly
        smaller accuracy scores. Similarly, a user can also click checkboxes under
        "Depth 3" to search for trees that use specific features in their
        second level.

development: >
  WebSHAP is written in <a href='https://www.typescriptlang.org'
  target='_blank' rel='noreferrer'>TypeScript</a>. The source code is available on
  <a href='https://github.com/poloclub/webshap' target='_blank' rel='noreferrer'>GitHub</a>.
  The entire app runs locally and privately in your web browser, and your model
  and data would never leave your machine. Anyone can easily access WebSHAP
  on their desktop or tablets, without the need of setting up dedicated servers or
  writing code.

team: >
  WebSHAP is created by <a href='https://zijie.wang/' target='_blank'
  rel='noreferrer'>Jay Wang</a> and <a href='https://www.cc.gatech.edu/~dchau'
  target='_blank' rel='noreferrer'>Polo Chau</a> at Georgia Tech.

contribute:
  - >
    If you have any questions or feedback, feel free to <a
    href='https://github.com/poloclub/webshap/issues' target='_blank' rel='noreferrer'>open an
    issue</a> or contact <a href='https://zijie.wang/' target='_blank' rel='noreferrer'>Jay
    Wang</a>.
  - >
    We‚Äôd love to hear your experience with WebSHAP! If you‚Äôd like to share
    (e.g., why do you use WebSHAP?), please reach out to us. WebSHAP is an open-source
    project, and we welcome <a href='https://github.com/poloclub/webshap/pulls'
    target='_blank' rel='noreferrer'>pull requests</a> for new feature
    implementations and bug fixes, etc.


cite:
  intro: >
    To learn more about WebSHAP, please read our <a
    href='https://arxiv.org/abs/2303.09545' target='_blank' rel='noreferrer'>research paper</a>
    (published at <a href='https://www2023.thewebconf.org'
    target='_blank' rel='noreferrer'>TheWebConf'23</a>). If you find WebSHAP useful for your research,
    please consider citing our paper. Thanks!
  bibtex: >
    @inproceedings{wangWebSHAPExplainingAny2023,
      title = {{{WebSHAP}}: {{Towards Explaining Any Machine Learning Models Anywhere}}},
      shorttitle = {{{WebSHAP}}},
      booktitle = {Companion {{Proceedings}} of the {{Web Conference}} 2023},
      author = {Wang, Zijie J. and Chau, Duen Horng},
      year = {2023},
      langid = {english}
    }
  title: >
    WebSHAP: Towards Explaining Any Machine Learning Models Anywhere
  paperLink: >
    https://arxiv.org/abs/2303.09545
  venue: >
    Companion Proceedings of the Web Conference 2023
  venueLink: >
    https://www2023.thewebconf.org
  authors:
    - name: Zijie J. Wang
      url: 'https://zijie.wang/'
    - name: Duen Horng (Polo) Chau
      url: 'https://www.cc.gatech.edu/~dchau'